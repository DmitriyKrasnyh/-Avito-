import requests
from bs4 import BeautifulSoup
import random
import time

# –°–ø–∏—Å–æ–∫ User-Agent'–æ–≤ –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
]

# –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞
HEADERS = {
    "User-Agent": random.choice(USER_AGENTS),
    "Accept-Language": "ru,en;q=0.9",
    "Referer": "https://www.google.com/",
}

# URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –≤–æ –í–ª–∞–¥–∏–º–∏—Ä–µ
URL = "https://www.avito.ru/vladimir/nedvizhimost"

# –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è cookies
session = requests.Session()
session.headers.update(HEADERS)

# –ó–∞–ø—Ä–æ—Å –∫ –ê–≤–∏—Ç–æ
response = session.get(URL)
time.sleep(random.uniform(2, 5))  # –î–µ–ª–∞–µ–º —Ä–∞–Ω–¥–æ–º–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞
if response.status_code == 200:
    soup = BeautifulSoup(response.text, "html.parser")

    # –ò—â–µ–º –±–ª–æ–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
    listings = soup.find_all("div", class_="iva-item-content-rejJg")  # –ö–ª–∞—Å—Å –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è!

    for item in listings[:5]:  # –í—ã–≤–µ–¥–µ–º 5 –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è —Ç–µ—Å—Ç–∞
        title = item.find("h3")
        price = item.find("meta", itemprop="price")
        link = item.find("a", class_="link-link-MbQDP")

        if title and price and link:
            print(f"üìå –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title.text.strip()}")
            print(f"üí∞ –¶–µ–Ω–∞: {price['content']} —Ä.")
            print(f"üîó –°—Å—ã–ª–∫–∞: https://www.avito.ru{link['href']}\n")
else:
    print(f"–û—à–∏–±–∫–∞ {response.status_code}: –ê–≤–∏—Ç–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª–æ –∑–∞–ø—Ä–æ—Å.")
